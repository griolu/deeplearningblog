{
  
    
        "post0": {
            "title": "Reflection on Trigger Me fast.ai and Future Plans",
            "content": "What I learned . So, yesterday&#39;s model was not a very well-made model. It seems that my objective didn&#39;t coincide with the way I trained it. Rather, it&#39;s objective is to differentiate between male vs. female, given an image of a Korean. When I gave it any image, it had a difficult time determining whether it was a Korean; for example, if I gave it an image of a bear, it would call it a Korean man with 90% probability; or, when I gave an image of a Black man, it predicted a Korean man with 99.95% probability. The model was trained on categories, so it would try to predict one of the categories. My thought was to have it learn the facial features of Koreans, but it seems like it just learned how to differentiate between genders and age of (most) human faces. The 80% probability threshold that I made may have also been too high. It wasn&#39;t able to detect if there was an Asian when I gave it another image of Michael Reeves or LilyPichu. It could also be that the data was quite biased to certain types of Korean faces. . What I&#39;ll do . I&#39;ll create a better model with the objective to predict on categories, not whether something is x. In other words, the plan of creating a model that can detect a cat or squirrel is scrapped. Tomorrow, I&#39;ll instead create a model that can tell you who the OfflineTV member is, given an image of an OfflineTV member. I can do a similar thing with yesterday, though, where if the probability is less than 50%, we can say they&#39;re not an OfflineTV member. One of the main issues would be getting the data because searching up &quot;DisguisedToast face&quot; may provide the same image of DisguisedToast&#39;s face reveal. Likewise, &quot;LilyPichu&quot; might just lead to images of her logo. But, we&#39;ll see how it goes. If all goes south, we can create a simpler model that can differentiate between a mouse and a keyboard :). .",
            "url": "https://griolu.github.io/deeplearningblog/jupyter/2021/01/29/Reflections.html",
            "relUrl": "/jupyter/2021/01/29/Reflections.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Trigger Me fast.ai",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() . from fastbook import * from fastai.vision.widgets import * . In this post, we&#39;re going to train a model that can detect if a Korean is in the photo and if so, give them a nice compliment. This model can be extrapolated to detect different races and give a more diverse range of compliments. However, to keep it simple, we will limit ourselves to one race. . Planning the model . We&#39;ll be using the DriveTrain approach to plan the model: &quot;use data not just to generate more data (in the form of predictions), but to produce actionable outcomes.&quot; I thought about how I should display it, and I think a mindmap will illustrate it well. I am going to be using CmapTools to create the mindmap. . Here is our resulting mindmap: . Getting the data . To get the data, we&#39;ll be using the Bing Image Search API. We should collect various kinds of data to account for different genders and age. . We have to set up our API key: . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;ENTER_API_KEY_HERE&#39;) . To limit the bias in our data, we&#39;ll create 4 categories of boy, girl, man, and woman. This will try to include different genders and age groups in our data. . types = &#39;man&#39;, &#39;boy&#39;, &#39;woman&#39;, &#39;girl&#39; path = Path(&#39;korean&#39;) . We create a path in our Colab Virtual Machine and donwload images that appear in the search query korean ...&quot;, where &quot;...&quot; is replaced by boy, girl, man, or woman by a for loop. . if not path.exists(): path.mkdir() for o in types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;korean {o}&#39;) download_images(dest, urls=results.attrgot(&#39;contentUrl&#39;)) . We can check if the images were downloaded correctly: . fns = get_image_files(path) . fns . (#561) [Path(&#39;korean/man/00000021.jpg&#39;),Path(&#39;korean/man/00000076.jpg&#39;),Path(&#39;korean/man/00000059.jpg&#39;),Path(&#39;korean/man/00000024.jpg&#39;),Path(&#39;korean/man/00000119.jpg&#39;),Path(&#39;korean/man/00000105.jpg&#39;),Path(&#39;korean/man/00000148.jpg&#39;),Path(&#39;korean/man/00000065.jpg&#39;),Path(&#39;korean/man/00000110.jpg&#39;),Path(&#39;korean/man/00000014.jpg&#39;)...] . We can check the folders were created correctly: . ls korean . boy/ girl/ man/ woman/ . Finally, we&#39;ll check if any images were downloaded incorrectly: . failed = verify_images(fns) failed . (#0) [] . We see there&#39;s none that downloaded incorrectly, but if there were, we would run this line: . failed.map(Path.unlink) . (#0) [] . We create a DataBlock for our data. . The blocks parameter requires a tuple of what type of object we want the independent and dependent variables to be. | The get_items parameter asks how we get the data (get_image_files returns all the images in a path recursively). | The splitter parameter is how we split our data into the training and validation sets. | The get_y parameter is how we get the labels for our data (parent_label takes the name of the folder the image is in). | The item_tfms parameter is how we transform every item in the data (Resize(128) takes every image and crops it to a 128x128 square). | . koreans = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = RandomSplitter(valid_pct = 0.2, seed = 42), get_y = parent_label, item_tfms = Resize(128) ) . We create our DataLoaders from the DataBlock: . dls = koreans.dataloaders(path) . We can peek at the images in the DataLoaders, specifically, from the validation set: . dls.valid.show_batch(max_n = 8, nrows = 2) . There&#39;s many ways to transform each item in the DataBlock object. There&#39;s Resize, Resize w/ ResizeMethod.Squish, Resize w/ ResizeMethod.Pad, RandomResizedCrop, and many more. What we want is RandomResizedCrop, which takes the same image and takes different sections of it of a specified size. Then, we want to apply a batch_tfms, which transforms a batch of items (of the same resolution) using the GPU, with aug_transforms, which will augment (distort; create random variations of) each item in the batch. Augmenting the data lets us have more data from the original data, while keeping the meaning of the data. . We apply the transformations and show a sample of the augmented data: . koreans = koreans.new(item_tfms = RandomResizedCrop(224, min_scale = 0.5), batch_tfms = aug_transforms()) dls = koreans.dataloaders(path) dls.train.show_batch(max_n = 8, nrows = 2, unique = True) . To train our model, we&#39;ll use cnn_learner with the resnet18 architecture. We&#39;ll be using the pretrained model and fine-tuning it with 4 epochs (run-through of the entire training set). . learn = cnn_learner(dls, resnet18, metrics = error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 2.160188 | 1.125887 | 0.482143 | 00:10 | . epoch train_loss valid_loss error_rate time . 0 | 1.507362 | 1.088920 | 0.437500 | 00:10 | . 1 | 1.359034 | 1.181865 | 0.428571 | 00:10 | . 2 | 1.234375 | 1.126657 | 0.446429 | 00:10 | . 3 | 1.172037 | 1.137771 | 0.428571 | 00:10 | . We see that our error rate is pretty high, but when we look at the confusion matrix, we realize why: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Majority of the errors are coming from the same gender being identified as different ages. This problem is largely an issue with how we got the data. Bing will have a difficult time getting Korean children just from boy vs. man or girl vs. woman. Our main objective is to identify Koreans, so predicting the wrong age is not very significant. Similarly, gender is also not significant since our categories were just to increase the diveristy of our data. . fast.ai also provides another way of checking the loss by showing the top losses. Here, we list the top 5 losses: . interp.plot_top_losses(5, nrows = 1) . They also provide the ImageClassiferCleaner GUI, which lets you conveniently clean the data: . cleaner = ImageClassifierCleaner(learn) cleaner . After adjusting the status of each image, you can run this block to commit the change: . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx, cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . Once you clean the data, you can retrain the model until you&#39;re satisfied. . There&#39;s a few limitations with how I created this model. I only provided images of Koreans, so it may lead the model to overfit and wrongly predict Korean for images of Chinese or Japanese people. Similarly, instead of getting the data ourselves, we used the Bing search API, which may provide biased data. For example, only providing people with healthy skin or in optimal picture positions. Nonetheless, it works to our advantage because we can change our objective to detect East Asians instead of just Koreans. . Exporting the trained model . When we export the model, we want to reset the path and use the .export function on the cnn_learner object: . path = Path() learn.export() . We can use the ls function to check if it exported the file: . ls . export.pkl gdrive/ korean/ sample_data/ . To download the .pkl file from Google Colaboratory, run this line: . from google.colab import files files.download(&#39;export.pkl&#39;) . Creating an online application . To create an online application with the model, we load the .pkl file with the load_learner function: . learn_inf = load_learner(path/&#39;export.pkl&#39;) . We can create an upload widget using IPython widjets for uploading images: . uploader = widgets.FileUpload() uploader . We upload an image and create PILImage object to store the uploaded image: . img = PILImage.create(uploader.data[0]) img . We run the .predict function with the image on our cnn_learner object: . learn_inf.predict(img) . (&#39;girl&#39;, tensor(1), tensor([3.6931e-05, 8.9920e-01, 1.6245e-04, 1.0060e-01])) . It returns a tuple of its most confident label, its index, and the probabilities for all the possible labels. . We can run this function to see all the labels (.vocab) of the cnn_learner object: . learn_inf.dls.vocab . [&#39;boy&#39;, &#39;girl&#39;, &#39;man&#39;, &#39;woman&#39;] . Now, we can create the application. First, we create the upload button: . upload_btn = widgets.FileUpload() upload_btn . We obtain the image from the upload button: . img = PILImage.create(upload_btn.data[-1]) img . We create an output widget to display our uploaded image in a small, thumbnail format: . out_img = widgets.Output() out_img.clear_output() with out_img: display(img.to_thumb(128, 128)) out_img . We obtain the values in the tuple: . pred, pred_idx, probs = learn_inf.predict(img) . We create a label to display the values in a neat format: . lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . We create a button for our cnn_learner to inference on the image: . run_btn = widgets.Button(description = &#39;Predict&#39;) run_btn . We create another label that will contain our &quot;trigger&quot;: . lbl_trigger = widgets.Label() . We have to create a function for when we click the predict button: . def run_btn_clicked(change): img = PILImage.create(upload_btn.data[-1]) out_img.clear_output() with out_img: display(img.to_thumb(128, 128)) pred, pred_idx, probs = learn_inf.predict(img) if probs[pred_idx] &lt; 0.8: lbl_pred.value = f&#39;Doesn &#39;t seem Asian to me.&#39; lbl_trigger.value = &quot;Come back later with an Asian.&quot; else: lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; if pred_idx == 0: # boy lbl_trigger.value = &quot;Have a nice day, boy.&quot; elif pred_idx == 1: # girl lbl_trigger.value = &quot;Have a nice day, girl.&quot; elif pred_idx == 2: # man lbl_trigger.value = &quot;Have a nice day, man.&quot; else: # woman lbl_trigger.value = &quot;Have a nice day, woman.&quot; run_btn.on_click(run_btn_clicked) . Finally, we create a vertical box to contain all our widgets: . VBox([widgets.Label(&#39;Select your human:&#39;), upload_btn, run_btn, out_img, lbl_pred, lbl_trigger]) . Deploying the notebook to an app via Voila and Binder . You can check out the model here. .",
            "url": "https://griolu.github.io/deeplearningblog/jupyter/2021/01/28/trigger-me-fast-ai.html",
            "relUrl": "/jupyter/2021/01/28/trigger-me-fast-ai.html",
            "date": " • Jan 28, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "New year, New Me",
            "content": "Hey there! . Introduction . Hi, I’m Geon and this blog is where I’ll be posting my progress with deep learning. I’ve delayed continuing on with fast.ai until the new year (and new semester). As of January 25, 2021, I finished my second-last semester of high school and am starting my last semester on February 2, 2021. In the second semester, I’ll be more free and have more time to work on this blog. I’m free of English and I’ll have a pretty nice schedule: a spare period, Chemistry, Math, and Physics. . What to expect . I’m going to be uploading my progress of learning deep learning with fast.ai. So, I’ll be using fastpages and Google Colaboratory to make these posts. I’ll accompany the code with text to make it easy to understand. This section is broad because I’m not too sure yet of what I’ll be making. Expect to see a new blog post every week or two on Saturday or Sunday. . Where to contact me (not that I check them often) . Twitter: @griolu . Instagram: @geon_youn .",
            "url": "https://griolu.github.io/deeplearningblog/markdown/2021/01/25/new-year-new-me.html",
            "relUrl": "/markdown/2021/01/25/new-year-new-me.html",
            "date": " • Jan 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "My To-Do List",
            "content": "My To-Do List . Brief . The fast.ai lesson recommends stopping in the middle of lesson 3 and adventuring into the world of deep learning. Their advice is why I am starting this blog. They have a top-down structure of learning, where they want us to learn the details when we need them rather than learning them all before doing any training and inferencing. So, before I continue lesson 3, there’s some things I want to do. We’ve only learned image recognition so far, so most of these will be similar, but more practice lets me learn more. . List . Asian detector (inspired by Michael Reeves and for lesson 2 Questionnaire 23). | Cat detector (because there’s a neighbourhood cat that comes to my house every day). | Squirrel detector (because squirrels also like to come to my house). | . Plan . I think each project will take me one to two weeks. So, I’ll probably finish this list and move on to the more technical lessons by the end of the first week of March. .",
            "url": "https://griolu.github.io/deeplearningblog/markdown/2021/01/06/my-to-do-list.html",
            "relUrl": "/markdown/2021/01/06/my-to-do-list.html",
            "date": " • Jan 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://griolu.github.io/deeplearningblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://griolu.github.io/deeplearningblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}